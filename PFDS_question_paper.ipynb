{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a code in python for analysing “mailbox”\n",
        "# dataset\n",
        "# a. . How many emails did I send during a given\n",
        "# timeframe?\n",
        "# b. At what times of the day do I send and receive\n",
        "# emails with Gmail?\n",
        "# c. What is the average number of emails per day?\n",
        "# d. What is the average number of emails per hour?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the mailbox data (replace 'mailbox_data.csv' with your actual file)\n",
        "try:\n",
        "    mailbox_df = pd.read_csv('mailbox_data.csv')  # Assumes CSV format; adjust if different\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'mailbox_data.csv' not found. Please provide the correct path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Assuming your mailbox data has columns like 'Date', 'Time', 'Sender', 'Recipient', 'Subject'\n",
        "\n",
        "# Convert 'Date' and 'Time' to datetime objects if they aren't already\n",
        "mailbox_df['Date'] = pd.to_datetime(mailbox_df['Date'])\n",
        "mailbox_df['Time'] = pd.to_datetime(mailbox_df['Time']).dt.time\n",
        "\n",
        "# a. Emails sent during a given timeframe\n",
        "def emails_sent_in_timeframe(start_date, end_date):\n",
        "    sent_emails = mailbox_df[(mailbox_df['Date'] >= start_date) & (mailbox_df['Date'] <= end_date) & (mailbox_df['Sender'] == 'my_email@gmail.com')]\n",
        "    return len(sent_emails)\n",
        "\n",
        "# Example usage\n",
        "start_date = '2024-01-01'\n",
        "end_date = '2024-01-31'\n",
        "num_emails_sent = emails_sent_in_timeframe(start_date, end_date)\n",
        "print(f\"Number of emails sent between {start_date} and {end_date}: {num_emails_sent}\")\n",
        "\n",
        "\n",
        "# b. Email sending/receiving times with Gmail\n",
        "def gmail_email_times(sender_or_recipient):\n",
        "  emails = mailbox_df[(mailbox_df['Sender'].str.contains('@gmail.com') & (mailbox_df['Sender'] == sender_or_recipient)) | (mailbox_df['Recipient'].str.contains('@gmail.com'))]\n",
        "  return emails['Time'].value_counts()\n",
        "\n",
        "gmail_times = gmail_email_times('my_email@gmail.com')\n",
        "print(\"\\nEmail times with Gmail:\\n\", gmail_times)\n",
        "\n",
        "\n",
        "# c. Average number of emails per day\n",
        "mailbox_df['DateOnly'] = mailbox_df['Date'].dt.date\n",
        "emails_per_day = mailbox_df.groupby('DateOnly').size()\n",
        "avg_emails_per_day = emails_per_day.mean()\n",
        "print(f\"\\nAverage emails per day: {avg_emails_per_day}\")\n",
        "\n",
        "# d. Average number of emails per hour\n",
        "mailbox_df['Hour'] = mailbox_df['Time'].apply(lambda t: t.hour)\n",
        "emails_per_hour = mailbox_df.groupby('Hour').size()\n",
        "avg_emails_per_hour = emails_per_hour.mean()\n",
        "print(f\"\\nAverage emails per hour: {avg_emails_per_hour}\")\n",
        "\n",
        "# (Optional) Visualization:\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(emails_per_hour.index, emails_per_hour.values)\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Number of Emails\")\n",
        "plt.title(\"Email Frequency by Hour\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mT1VOYfmZHIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "C12lBehFYH2B",
        "outputId": "77ce1702-00d4-4f2a-f789-be2dd500244b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'titanic.csv' not found. Please ensure the file is in the current directory or provide the correct path.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'titanic_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-27016aa8c7ef>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# A. Handle missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Identify columns with missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing Values:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'titanic_df' is not defined"
          ]
        }
      ],
      "source": [
        "# prompt: Write a code in python for “titanic” dataset for the\n",
        "# following:-\n",
        "# A, Check for missing values and handle them.\n",
        "# B. Check for duplicates and handle them.\n",
        "# C. Check for outliers and handle them.\n",
        "# D. Check for data types and convert them if\n",
        "# necessary.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "# Replace 'titanic.csv' with the actual path to your dataset file\n",
        "try:\n",
        "    titanic_df = pd.read_csv('titanic.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'titanic.csv' not found. Please ensure the file is in the current directory or provide the correct path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# A. Handle missing values\n",
        "# Identify columns with missing values\n",
        "missing_values = titanic_df.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "\n",
        "# Impute missing 'Age' with the median\n",
        "imputer_age = SimpleImputer(strategy='median')\n",
        "titanic_df['Age'] = imputer_age.fit_transform(titanic_df[['Age']])\n",
        "\n",
        "# Impute missing 'Embarked' with the most frequent value\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')\n",
        "titanic_df['Embarked'] = imputer_embarked.fit_transform(titanic_df[['Embarked']])\n",
        "\n",
        "\n",
        "# Fill missing 'Cabin' with 'Unknown' (or another appropriate strategy)\n",
        "titanic_df['Cabin'].fillna('Unknown', inplace=True)\n",
        "\n",
        "\n",
        "# Drop rows where 'Fare' is missing (if any exist)\n",
        "titanic_df.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "\n",
        "# B. Handle duplicates\n",
        "# Check for duplicate rows\n",
        "print(\"\\nDuplicate Rows:\", titanic_df.duplicated().sum())\n",
        "\n",
        "# Remove duplicate rows (if any exist)\n",
        "titanic_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# C. Handle outliers\n",
        "# Handle outliers in 'Fare' using IQR method\n",
        "Q1_fare = titanic_df['Fare'].quantile(0.25)\n",
        "Q3_fare = titanic_df['Fare'].quantile(0.75)\n",
        "IQR_fare = Q3_fare - Q1_fare\n",
        "\n",
        "lower_bound_fare = Q1_fare - 1.5 * IQR_fare\n",
        "upper_bound_fare = Q3_fare + 1.5 * IQR_fare\n",
        "\n",
        "titanic_df = titanic_df[(titanic_df['Fare'] >= lower_bound_fare) & (titanic_df['Fare'] <= upper_bound_fare)]\n",
        "\n",
        "# Consider other columns like Age for outlier treatment as needed\n",
        "\n",
        "# D. Handle data types\n",
        "# Convert 'Age' and 'Fare' to numeric if necessary\n",
        "titanic_df['Age'] = pd.to_numeric(titanic_df['Age'])\n",
        "titanic_df['Fare'] = pd.to_numeric(titanic_df['Fare'])\n",
        "\n",
        "# Example of handling categorical features: Convert 'Sex' and 'Embarked' to numerical using one-hot encoding\n",
        "titanic_df = pd.get_dummies(titanic_df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "\n",
        "# Print information about the cleaned data\n",
        "print(\"\\nData types:\\n\", titanic_df.dtypes)\n",
        "\n",
        "print(\"\\nDescriptive Statistics:\\n\", titanic_df.describe())\n",
        "\n",
        "print(\"\\nShape of the cleaned dataframe: \", titanic_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a code in python for “sales.csv” following\n",
        "# a. Add a column “Totalprice” paid for each\n",
        "# purchase.\n",
        "# b. List the transaction that exceeds 3,000,000 for\n",
        "# “Totalprice”\n",
        "# c. Find the sales in the year “2015” and “1973”\n",
        "# d. List the clients established before the year\n",
        "# “2001”\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the sales data\n",
        "try:\n",
        "    sales_df = pd.read_csv('sales.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'sales.csv' not found. Please provide the correct path.\")\n",
        "    exit()\n",
        "\n",
        "# Assuming your sales data has columns like 'Quantity', 'UnitPrice'\n",
        "\n",
        "# a. Add a 'TotalPrice' column\n",
        "sales_df['TotalPrice'] = sales_df['Quantity'] * sales_df['UnitPrice']\n",
        "\n",
        "# b. Transactions exceeding 3,000,000\n",
        "expensive_transactions = sales_df[sales_df['TotalPrice'] > 3000000]\n",
        "print(\"Transactions exceeding 3,000,000:\\n\", expensive_transactions)\n",
        "\n",
        "# c. Sales in 2015 and 1973\n",
        "# Assuming you have a 'Date' column in your dataset (replace 'InvoiceDate' if different)\n",
        "# Convert 'InvoiceDate' to datetime objects if necessary\n",
        "if not pd.api.types.is_datetime64_any_dtype(sales_df['InvoiceDate']):\n",
        "    sales_df['InvoiceDate'] = pd.to_datetime(sales_df['InvoiceDate'])\n",
        "\n",
        "sales_2015 = sales_df[sales_df['InvoiceDate'].dt.year == 2015]\n",
        "sales_1973 = sales_df[sales_df['InvoiceDate'].dt.year == 1973]\n",
        "\n",
        "print(\"\\nSales in 2015:\\n\", sales_2015)\n",
        "print(\"\\nSales in 1973:\\n\", sales_1973)\n",
        "\n",
        "# d. Clients established before 2001\n",
        "# Assuming you have a 'ClientEstablishedDate' column\n",
        "# Convert to datetime if it's not already\n",
        "if not pd.api.types.is_datetime64_any_dtype(sales_df['ClientEstablishedDate']):\n",
        "    sales_df['ClientEstablishedDate'] = pd.to_datetime(sales_df['ClientEstablishedDate'])\n",
        "\n",
        "old_clients = sales_df[sales_df['ClientEstablishedDate'].dt.year < 2001]\n",
        "print(\"\\nClients established before 2001:\\n\", old_clients)"
      ],
      "metadata": {
        "id": "lCWqCfehZlBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use “automobile.csv” to perform following tasks in\n",
        "# python\n",
        "# a. group this dataset on the basis of the body-style\n",
        "# column\n",
        "# b. print the values contained in that group that\n",
        "# have the body-style value of sedan\n",
        "# c. Print the composite group using feature “bodystyle” and “drive-wheels”\n",
        "# d. Group the data frame df by body-style and drivewheels and extract length, height and price from\n",
        "# each group\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the automobile dataset\n",
        "try:\n",
        "    df = pd.read_csv('automobile.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'automobile.csv' not found. Please provide the correct path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# a. Group by body-style\n",
        "body_style_groups = df.groupby('body-style')\n",
        "\n",
        "# b. Print values for 'sedan' body-style\n",
        "print(\"Values for Sedan body-style:\")\n",
        "print(body_style_groups.get_group('sedan'))\n",
        "\n",
        "# c. Composite group by body-style and drive-wheels\n",
        "composite_group = df.groupby(['body-style', 'drive-wheels'])\n",
        "print(\"\\nComposite group by body-style and drive-wheels:\")\n",
        "print(composite_group.size().unstack(fill_value=0))\n",
        "\n",
        "# d. Group and extract length, height, and price\n",
        "grouped = df.groupby(['body-style', 'drive-wheels'])\n",
        "extracted_features = grouped['length', 'height', 'price'].agg(['mean', 'max', 'min'])\n",
        "print(\"\\nExtracted length, height, and price:\")\n",
        "extracted_features"
      ],
      "metadata": {
        "id": "ijUvDM-MZuJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a code in python for “titanic” dataset for the\n",
        "# following:-\n",
        "# a. find out the percentages of women and men who\n",
        "# survived the disaster\n",
        "# b. visualize this information using the survival\n",
        "# numbers of males and females\n",
        "# c. visualize the number of survivors and deaths\n",
        "# from different Pclasses\n",
        "# d. multivariate analysis on the Titanic dataset using\n",
        "# the Survived, Pclass, Fear, and Age variables:\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# a. Percentage of women and men who survived\n",
        "women_survived = titanic_df[(titanic_df['Sex_male'] == 0) & (titanic_df['Survived'] == 1)]\n",
        "men_survived = titanic_df[(titanic_df['Sex_male'] == 1) & (titanic_df['Survived'] == 1)]\n",
        "\n",
        "total_women = titanic_df[titanic_df['Sex_male'] == 0]\n",
        "total_men = titanic_df[titanic_df['Sex_male'] == 1]\n",
        "\n",
        "women_survival_percentage = (len(women_survived) / len(total_women)) * 100\n",
        "men_survival_percentage = (len(men_survived) / len(total_men)) * 100\n",
        "\n",
        "print(f\"Women survival percentage: {women_survival_percentage:.2f}%\")\n",
        "print(f\"Men survival percentage: {men_survival_percentage:.2f}%\")\n",
        "\n",
        "\n",
        "# b. Visualize survival numbers of males and females\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(['Women', 'Men'], [len(women_survived), len(men_survived)])\n",
        "plt.title('Survival Numbers by Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Number of Survivors')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# c. Visualize survivors and deaths from different Pclasses\n",
        "survived_by_pclass = titanic_df.groupby('Pclass')['Survived'].sum()\n",
        "deaths_by_pclass = titanic_df.groupby('Pclass')['Survived'].count() - survived_by_pclass\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(survived_by_pclass.index, survived_by_pclass.values, label='Survived')\n",
        "plt.bar(deaths_by_pclass.index, deaths_by_pclass.values, bottom=survived_by_pclass.values, label='Deaths')\n",
        "plt.xlabel('Pclass')\n",
        "plt.ylabel('Number of Passengers')\n",
        "plt.title('Survivors and Deaths by Pclass')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# d. Multivariate analysis\n",
        "# (Example: Correlation matrix)\n",
        "multivariate_df = titanic_df[['Survived', 'Pclass', 'Fare', 'Age']]  # Select variables for analysis\n",
        "correlation_matrix = multivariate_df.corr()\n",
        "\n",
        "print(\"Correlation matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualization using a heatmap\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Titanic Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MOwE8w_pZ360"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}